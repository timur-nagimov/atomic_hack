{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-22T10:43:10.734238Z","iopub.execute_input":"2023-10-22T10:43:10.734583Z","iopub.status.idle":"2023-10-22T10:43:11.070557Z","shell.execute_reply.started":"2023-10-22T10:43:10.734555Z","shell.execute_reply":"2023-10-22T10:43:11.069281Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/atomic-new/model_300dim.pkl\n/kaggle/input/atomic-new/large_data.csv\n/kaggle/input/atomic-new/small_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rdkit mol2vec gensim -q\n","metadata":{"execution":{"iopub.status.busy":"2023-10-22T10:43:47.554003Z","iopub.execute_input":"2023-10-22T10:43:47.554331Z","iopub.status.idle":"2023-10-22T10:44:10.683331Z","shell.execute_reply.started":"2023-10-22T10:43:47.554303Z","shell.execute_reply":"2023-10-22T10:44:10.682274Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: mol2vec in /opt/conda/lib/python3.10/site-packages (0.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mol2vec) (1.23.5)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (from mol2vec) (4.3.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from mol2vec) (4.66.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from mol2vec) (1.3.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from mol2vec) (2.0.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mol2vec) (3.7.2)\nRequirement already satisfied: IPython in /opt/conda/lib/python3.10/site-packages (from mol2vec) (8.14.0)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from mol2vec) (0.12.2)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim->mol2vec) (1.11.2)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim->mol2vec) (6.3.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (0.18.2)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (0.1.6)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (3.0.38)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (2.15.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (5.9.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from IPython->mol2vec) (4.8.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mol2vec) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->mol2vec) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->mol2vec) (2023.3)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->IPython->mol2vec) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->IPython->mol2vec) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->mol2vec) (0.2.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mol2vec) (1.16.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython->mol2vec) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython->mol2vec) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->IPython->mol2vec) (0.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\nimport pandas as pd\nfrom rdkit import Chem\nfrom rdkit.Chem import RDKFingerprint\nfrom mol2vec.features import mol2alt_sentence, MolSentence, DfVec, sentences2vec\nfrom gensim.models import word2vec\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2023-10-22T10:47:29.120729Z","iopub.execute_input":"2023-10-22T10:47:29.121101Z","iopub.status.idle":"2023-10-22T10:47:29.128658Z","shell.execute_reply.started":"2023-10-22T10:47:29.121074Z","shell.execute_reply":"2023-10-22T10:47:29.127456Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class MolDataset(Dataset):\n    \"\"\"\n    Dataset for molecular data\n    \"\"\"\n    def __init__(self, X, y, X_func=lambda a: a.to_numpy(dtype=np.int32), X_get_func=lambda a: a):\n        \"\"\"\n        Initiates a dataset.\n        \n        :param X: all training features\n        :param y: all ground truth labels\n        :param X_func: a function that converts X into the form it should be stored as\n        :param X_get_func: a function that processes an element in X to the form that should be fed to a model as input\n        \"\"\"\n        self.X = X_func(X)\n        self.y = y.to_numpy(dtype=np.int32)\n        self.f = X_get_func\n    \n    def __getitem__(self, index):\n        return self.f(self.X[index]), self.y[index]\n    \n    def __len__(self):\n        return self.y.shape[0]\n\n    \ndef torch_accuracy(loader, model, device, conv=False):\n    \"\"\"\n    Evaluates the accuracy of a pytorch model\n    \n    :param loader: a DataLoader instance\n    :param model: a torch.nn.Module\n    :param device: 'cuda' or 'cpu'\n    :param conv: whether this model is a convolution model\n    :return: the accuracy of the input model evaluated on the dataset in the dataloader\n    \"\"\"\n    size = len(loader.dataset)\n    correct = 0\n    model.eval()\n    with torch.no_grad():\n        for X, y in loader:\n            if conv:\n                X = X.unsqueeze(1)\n            X, y = X.to(device).float(), y.to(device).long()\n            pred = model(X)\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    \n    return correct / size\n\n\ndef train(dataloader, model, loss_fn, optimizer, device, print_loss=True, conv=False):\n    \"\"\"\n    Trains a pytorch model on a dataset.\n    \n    :param dataloader: a DataLoader instance with the data to be trained on\n    :param model: a torch.nn.Module\n    :param loss_fn: the loss function\n    :param optimizer: the optimizer used for training\n    :param device: 'cuda' or 'cpu'\n    :param print_loss: whether to print the loss value during training\n    :param conv: whether this model is a convolution model\n    :return: a list of loss values calcualted during training\n    \"\"\"\n    losses = []\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        if conv:\n            X = X.unsqueeze(1)\n        X, y = X.to(device).float(), y.to(device).long()\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        losses.append(loss.item())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if print_loss and batch % 100 == 0:\n            loss = loss.item()\n            print(f'loss: {loss}')\n    return losses\n\n\ndef init_weights(m):\n    \"\"\"\n    He initialization for linear layers in a model\n    \n    :param m: a layer in a neural network\n    \"\"\"\n    if isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n        m.bias.data.fill_(0.01)\n\n         \n            \nclass MyModel:\n    \"\"\"\n    Wrapper type for all neural network models used in this project.\n    Specific models should subclass this class and specify parameters.\n    \"\"\"\n    def __init__(self, model, X, y, batch_size, partition, learning_rate, reg_strength=0, seed=None, X_func=lambda a: a.to_numpy(dtype=np.int32), X_get_func=lambda a: a, conv=False):\n        \"\"\"\n        Builds a neural network model.\n        \n        :param model: the model to be trained; an instance of a torch.nn.Module\n        :param X: all input features (train and test sets)\n        :param y: all ground truth labels (train and test sets)\n        :param batch_size: mini batch size\n        :param partition: ratio of the input dataset to use as the test set\n        :param learning_rate: learning rate\n        :param reg_strength: strength of L2 regularization\n        :param seed: for controlling how the dataset is split into train and test sets\n        :param X_func: a function that converts X into the form it should be stored as\n        :param X_get_func: a function that processes an element in X to the form that should be fed to a model as input\n        :param conv: whether this model is a convolution model\n        \"\"\"        \n        \n        # split into train and test sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            X, y, test_size=partition, random_state=seed)\n        \n        # initialize datasets and dataloaders\n        self.train_dset = MolDataset(self.X_train, self.y_train, X_func, X_get_func)\n        self.test_dset = MolDataset(self.X_test, self.y_test, X_func, X_get_func)\n        self.train_loader = DataLoader(self.train_dset, batch_size=batch_size, shuffle=True)\n        self.test_loader = DataLoader(self.test_dset, batch_size=batch_size, shuffle=True)\n        \n        # initialize model\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model = model.to(self.device)\n        self.model.apply(init_weights)\n        \n        # loss function and optimizer\n        self.loss = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=reg_strength)\n        \n        self.conv = conv\n        \n    def train(self, epochs, print_loss=True):\n        \"\"\"\n        Trains the model\n        \n        :param epochs: how many epochs to train for\n        :param print_loss: whether to print the loss values during training\n        :return: list of loss values, train accuracies, and test accuracies as training progressed\n        \"\"\"\n        losses = []\n        train_accuracies = []\n        test_accuracies = []\n        for t in range(epochs):\n            if print_loss:\n                print(f'Epoch {t + 1}\\n--------------------')\n            losses += train(self.train_loader, self.model, self.loss, self.optimizer, self.device, print_loss, self.conv)\n            accuracies = self.evaluate()\n            train_accuracies.append(accuracies[0])\n            test_accuracies.append(accuracies[1])\n        return losses, train_accuracies, test_accuracies\n   \n    def evaluate(self):\n        \"\"\"\n        Evaluates the accuracy of the model\n        \n        :return: the current train and test accuracies\n        \"\"\"\n        train_accuracy = torch_accuracy(self.train_loader, self.model, self.device, self.conv)\n        test_accuracy = torch_accuracy(self.test_loader, self.model, self.device, self.conv)\n        return train_accuracy, test_accuracy\n\n    \ndef plot(data, xlabel, ylabel):\n    \"\"\"\n    Plots a series of data\n    \n    :param data: a series of values to plot\n    :param xlabel: label of x axis\n    :param ylabel: label of y axis\n    \"\"\"\n    plt.plot(list(range(len(data))), data)\n    plt.ylabel(ylabel)\n    plt.xlabel(xlabel)\n    plt.show()\n    \n    \ndef evaluate_model(model, epochs, print_loss=True, plot_metrics=False):\n    \"\"\"\n    Trains a neural network and evaluates it accuracy\n    \n    :param model: the model to evaluate; an instance of MyModel\n    :param epochs: how many epochs to train for\n    :param print_loss: whether to print the loss values during training\n    :param plot_metrics: whether to plot the loss and accuracy values\n    :return: train and test accuracies of the model\n    \"\"\"\n    losses, train_accuracies, test_accuracies = model.train(epochs, print_loss)\n    train_accuracy, test_accuracy = model.evaluate()\n    \n    if plot_metrics:\n        plot(losses, '# of iterations', 'loss')\n        plot(train_accuracies, 'epoch', 'train accuracy')\n        plot(test_accuracies, 'epoch', 'test accuracy')\n        \n    return train_accuracy, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-10-22T10:47:29.806395Z","iopub.execute_input":"2023-10-22T10:47:29.806745Z","iopub.status.idle":"2023-10-22T10:47:29.831643Z","shell.execute_reply.started":"2023-10-22T10:47:29.806718Z","shell.execute_reply":"2023-10-22T10:47:29.829781Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def fingerprint(smiles):\n    \"\"\"\n    Generates molecular fingerprint for a molecule.\n    \n    :param smiles: SMILES string of a molecule\n    :return: the corresponding 2048-bit vector fingerprint, as a np array\n    \"\"\"\n    mol = Chem.MolFromSmiles(smiles)\n    return np.array(RDKFingerprint(mol))\n\n\ndef generate_word_embedding(data_file, out_file, model_file='/kaggle/input/atomic-new/model_300dim.pkl'):\n    \"\"\"\n    Uses a pre-trained model to generate word embeddings for a list of molecules.\n    Prepends the embeddings as columns to the input data file \n    and writes an output csv file.\n    \n    :param data_file: path to a csv data file containing a 'SMILES' column of SMILES strings\n    :param out_file: path to the output csv file\n    :param model_file: path to a pre-trained model\n    :return: data from the new csv file written to disk, as a pandas DataFrame\n    \"\"\"\n    data = pd.read_csv(data_file)\n    mol = [Chem.MolFromSmiles(i) for i in data['text']]\n    sentence = [MolSentence(mol2alt_sentence(i, radius=1)) for i in mol]\n    w2v_model = word2vec.Word2Vec.load(model_file)\n    embedding = [DfVec(x) for x in sentences2vec(sentence, w2v_model)]\n    data_mol2vec = np.array([x.vec for x in embedding])\n    data_mol2vec = pd.DataFrame(data_mol2vec)\n    new_data = pd.concat([data_mol2vec, data], axis=1)\n    new_data.to_csv(out_file, index=False)\n    return new_data\n\n\nSMILES_CHARS = [' ',\n                '#', '%', '(', ')', '+', '-', '.', '/',\n                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n                '=', '@',\n                'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n                'R', 'S', 'T', 'V', 'X', 'Z',\n                '[', '\\\\', ']',\n                'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n                't', 'u']\nsmi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\nindex2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n\n\ndef smiles_encoder(smiles, maxlen=500):\n    \"\"\"\n    Generates one-hot encodings of a SMILES string.\n    Each column represents a character.\n    Pads with columns of zeros to some maximum length.\n    \n    :param smiles: SMILES string of a molecule\n    :param maxlen: maximum legnth allowed for SMILES strings; shorter strings are padded to this length\n    :return: the one-hot encoding of the SMILES string as a np array\n    \"\"\"\n    X = np.zeros((len(SMILES_CHARS), maxlen))\n    for i, c in enumerate(smiles):\n        X[smi2index[c], i] = 1\n    return X\n\n\ndef smiles_decoder(X):\n    \"\"\"\n    Converts a one-hot encoding of a SMILES string back to the original 1D string.\n    \n    :param X: one-hot encoding of a molecule\n    :return: corresponding SMILES string\n    \"\"\"\n    smi = ''\n    X = X.argmax(axis=0)\n    for i in X:\n        smi += index2smi[i]\n    return smi.strip()\n\n\ndef accuracy(truth, predicted):\n    \"\"\"\n    Calculates the accuracy of the predictions of a classifier\n    \n    :param truth: list of ground truth labels\n    :param predicted: list of predicted labels\n    :returns: accuracy of the prediction\n    \"\"\"\n    return np.sum(predicted == truth) / len(predicted)\n\n\ndef test_log_reg(X, y, partition):\n    \"\"\"\n    Runs logistic regression on a dataset and evaluates its accuracy.\n    \n    :param X: all input features (train and test sets)\n    :param y: all ground truth labels (train and test sets)\n    :param partition: percentage of the input dataset to be used as the test set\n    :return: train and test accuracies of logsitic regression\n    \"\"\"\n    clf = LogisticRegression(max_iter=2000)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=partition)\n    clf.fit(X_train, y_train)\n    train_preds = clf.predict(X_train)\n    test_preds= clf.predict(X_test)\n    return accuracy(y_train, train_preds), accuracy(y_test, test_preds)\n\n\ndef test_model(model, data, partition, runs, epochs, print_loss=True, plot=False, X=None, y=None):\n    \"\"\"\n    Tests a classifier model on a dataset and evaluates its accuracy.\n    Prints out the train and test accuracies.\n    \n    :param model: a subclass of torch_utils.MyModel; this object will be called to construct a DL model instance; or 'logreg' for logistic regression\n    :param data: the entire dataset (train and test); not used if model is 'logreg'\n    :param partition: percentage of the input dataset to be used as the test set\n    :param runs: how many times the model is evaluated; the reported accuracies are averaged across all runs\n    :param epochs: number of training epochs; not used if model is 'logreg'\n    :param print_loss: whether to print loss values during training; not used if model is 'logreg'\n    :param plot: whether to plot loss and accuracy values after each run; not used if model is 'logreg'\n    :param X: all input features (train and test sets); only used if model is 'logreg'\n    :param y: all ground truth labels (train and test sets); only used if model if 'logreg'\n    :return: train and test accuracies of the model averaged across all runs\n    \"\"\"\n    train_accuracies, test_accuracies = [], []\n    \n    for _ in range(runs):\n        if model == 'logreg':\n            train_accuracy, test_accuracy = test_log_reg(X, y, partition)\n        else:\n            \n            m = model(data, partition)\n            m = m.to(device)\n            train_accuracy, test_accuracy = evaluate_model(m, epochs, print_loss, plot_metrics=plot)\n        \n        train_accuracies.append(train_accuracy)\n        test_accuracies.append(test_accuracy)\n        \n    train_mean = np.mean(train_accuracies)\n    test_mean = np.mean(test_accuracies)\n    print(f'Train accuracy mean: {train_mean}')\n    print(f'Test accuracy mean: {test_mean}')\n        \n    return train_mean, test_mean","metadata":{"execution":{"iopub.status.busy":"2023-10-22T10:47:30.696382Z","iopub.execute_input":"2023-10-22T10:47:30.696749Z","iopub.status.idle":"2023-10-22T10:47:30.713754Z","shell.execute_reply.started":"2023-10-22T10:47:30.696720Z","shell.execute_reply":"2023-10-22T10:47:30.712745Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class DNNModel(nn.Module):\n    def __init__(self):\n        super(DNNModel, self).__init__()\n        self.layers = nn.Sequential(nn.Linear(2048, 500),\n                                    nn.BatchNorm1d(500),\n                                    nn.ReLU(),\n                                    nn.Linear(500, 200),\n                                    nn.BatchNorm1d(200),\n                                    nn.ReLU(),\n                                    nn.Linear(200, 3))\n    \n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\n    \nclass FingerprintDNNModel(MyModel):\n    def __init__(self, data, partition):\n        super(FingerprintDNNModel, self).__init__(DNNModel(), data['text'], data['target'], 128, \n                                          partition, 0.0001, X_func=lambda a: a.tolist(), \n                                          X_get_func=lambda a: fingerprint(a))\n\n        \nclass ConvModel(nn.Module):\n    def __init__(self):\n        super(ConvModel, self).__init__()\n        self.layers = nn.Sequential(nn.Conv1d(1, 10, 7),\n                                    nn.ReLU(),\n                                    nn.MaxPool1d(10),\n                                    nn.Conv1d(10, 30, 7),\n                                    nn.ReLU(),\n                                    nn.MaxPool1d(10),\n                                    nn.Flatten(),\n                                    nn.Linear(570, 50),\n                                    nn.ReLU(),\n                                    nn.Linear(50, 20),\n                                    nn.ReLU(),\n                                    nn.Linear(20, 3))\n    \n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\n    \nclass FingerprintConvModel(MyModel):\n    def __init__(self, data, partition):\n        super(FingerprintConvModel, self).__init__(ConvModel(), data['text'], data['target'], 128, \n                                          partition, 0.001, X_func=lambda a: a.tolist(), \n                                          X_get_func=lambda a: fingerprint(a), conv=True)\n\n        \nif __name__ == '__main__':\n    large_data = pd.read_csv('/kaggle/input/atomic-new/large_data.csv')\n    small_data = pd.read_csv('/kaggle/input/atomic-new/small_data.csv')\n    \n    print('Testing logistic regression on large dataset')\n    test_model('logreg', None, 0.06, 1, 0, X=np.array([fingerprint(i) for i in large_data['text']]), y=large_data['target'])\n    print('Testing logistic regression on small dataset')\n    test_model('logreg', None, 0.2, 50, 0, X=np.array([fingerprint(i) for i in small_data['text']]), y=small_data['target'])\n    \n    \n    print('Testing DNN model on large dataset')\n    test_model(FingerprintDNNModel, large_data, 0.06, 1, 10, print_loss=True, plot=False)\n   \n    print('Testing DNN model on small dataset')\n    test_model(FingerprintDNNModel, small_data, 0.2 , 50, 10, print_loss=True, plot=False)\n    \n    print('Testing 1D CNN model on large dataset')\n    test_model(FingerprintConvModel, large_data, 0.06, 1, 10, print_loss=True, plot=False)\n   \n    print('Testing 1D CNN model on small dataset')\n    test_model(FingerprintConvModel, small_data, 0.2 , 50, 10, print_loss=True, plot=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-22T10:47:31.423089Z","iopub.execute_input":"2023-10-22T10:47:31.423885Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Testing logistic regression on large dataset\n","output_type":"stream"},{"name":"stderr","text":"[10:47:31] Conflicting single bond directions around double bond at index 55.\n[10:47:31]   BondStereo set to STEREONONE and single bond directions set to NONE.\n[10:49:26] Conflicting single bond directions around double bond at index 7.\n[10:49:26]   BondStereo set to STEREONONE and single bond directions set to NONE.\n","output_type":"stream"},{"name":"stdout","text":"Train accuracy mean: 0.9496695326665496\nTest accuracy mean: 0.9111314704535044\nTesting logistic regression on small dataset\n","output_type":"stream"},{"name":"stderr","text":"[10:51:46] Conflicting single bond directions around double bond at index 55.\n[10:51:46]   BondStereo set to STEREONONE and single bond directions set to NONE.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}